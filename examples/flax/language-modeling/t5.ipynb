{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/workdir/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_no/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from t5_tokenizer_model import SentencePieceUnigramTokenizer\n",
    "\n",
    "vocab_size = 32000\n",
    "\n",
    "# size of the input data\n",
    "input_sentence_size = None\n",
    "\n",
    "cache_dir=\"/workdir/.cache/huggingface/datasets\"\n",
    "\n",
    "# using a tiny bit of the dataset to train a tokenizer\n",
    "ds = datasets.load_dataset(\"oscar\",  \n",
    "    name=\"unshuffled_deduplicated_no\", \n",
    "    cache_dir=cache_dir, \n",
    "    split=\"train[:100]\"\n",
    "    )\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "def batch_iterator(ds, input_sentence_size=None):\n",
    "    if input_sentence_size==None:\n",
    "        input_sentence_size = len(ds)\n",
    "    batch_size = 100\n",
    "    for i in range(0, input_sentence_size, batch_size):\n",
    "        yield ds[i: i+batch_size][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(ds),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "t5_config_dir = \"/workdir/norwegian-t5-base/\"\n",
    "if not os.path.exists(t5_config_dir):\n",
    "    os.makedirs(t5_config_dir)\n",
    "\n",
    "tokenizer.save(os.path.join(t5_config_dir, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6152"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import T5Config\n",
    "\n",
    "config = T5Config.from_pretrained(\"google/t5-v1_1-base\", vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained(t5_config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 experiments replicating\n",
    "I should answer the following\n",
    "questions when looking at the implementation:\n",
    "\n",
    "- how span-masking is implemented <\n",
    "- exactly how loss is computed.\n",
    "    - implementation details of decoder\n",
    "    - [do the most simple example](Dummy Examples)\n",
    "- input and labeled of span-masked pre-training data\n",
    "- modeling details / optimizer / learning rate scheduling\n",
    "- how they implemented metrics logging. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Config\n",
    "# use rust based tokenizer\n",
    "cache_dir=\"/workdir/norwegian-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cache_dir, \n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    use_auth_token=None\n",
    ")\n",
    "\n",
    "config = T5Config.from_pretrained(\n",
    "    \"/workdir/norwegian-t5-base/\",\n",
    "    cache_dir=cache_dir,\n",
    "    vocab_size = len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Span-masked language modeling\n",
    "\n",
    "Spans of the input sequence are masked by so-called sentinel tokens (unique mask tokens) and the output sequence\n",
    "is formed as a concatenation of the same sentinel tokens and the real token that has been masked out. For example, if the input sequence is \n",
    "\n",
    "`The bad dog ruined my sleep`\n",
    "\n",
    "We can mask out `bad dog` and ask the model to predict it. The sequence we will feed to the encoder is \n",
    "\n",
    "`The <extra_id_0> ruined my sleep`\n",
    "\n",
    "The label we use to compute the loss is \n",
    "\n",
    "`<extra_id_0> bad dog <extra_id_1>`\n",
    "\n",
    "T5-like span masked language models fuse the consecutively masked tokens to a single sentinel token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6152], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<extra_id_0>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad token id:  0\n",
      "decoder start token id:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"pad token id: \", config.pad_token_id)\n",
    "print(\"decoder start token id: \", config.decoder_start_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are needed to create span-masked \n",
    "\n",
    "- tokenizer: PreTrainedTokenizerBase\n",
    "A pretrained tokenizer with all the extra id stuff\n",
    "\n",
    "- noise_density: float = 0.15 (data_args.mlm_probability,)\n",
    "The probablity of mask out a token\n",
    "\n",
    "- mean_noise_span_length: float (data_args.mean_noise_span_length,)\n",
    "Average size of the masked span\n",
    "\n",
    "- input_length: int\n",
    "Maximum input sequence length. Defined as \n",
    "```\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "```\n",
    "\n",
    "- target_length: int\n",
    "Target sequence length. This quantity depends on the `max_seq_length` and is computed\n",
    "by `def compute_input_and_target_lengths`\n",
    "\n",
    "\n",
    "- pad_token_id: int (model.config.pad_token_id,)\n",
    "id for the pad token. 0 for T5\n",
    "\n",
    "- decoder_start_token_id: int (model.config.decoder_start_token_id)\n",
    "start token for sequence feed into decoder. 0 for T5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input type to `FlaxDataCollatorForT5MLM`\n",
    "Span-mask is implemeted in `FlaxDataCollatorForT5MLM`, the input to the `__call__` method is \n",
    "a `List[Dict[str, np.ndarray]]`, i.e. it is a batch of input data, each of them has the signiture:\n",
    "```\n",
    "{\n",
    "    \"input_ids\": [..., token ids,...],\n",
    "    \"masks\": np.array,\n",
    "}\n",
    "```\n",
    "The instance of `FlaxDataCollatorForT5MLM` is refered as\n",
    "`data_collator` in the code. I can check what kind of input is fed into `data_collator` at line 895. \n",
    "The `tokenized_dataset` object from which we generate the batch of data is a standard interface in HuggingFace\n",
    "\n",
    "### Hugging Face Dataset\n",
    "The `tokenized_dataset` is defined as follwing. This pattern is the same for many language model training usages\n",
    "in Hugging Face\n",
    "```python\n",
    "# 557\n",
    "datasets = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    data_args.dataset_config_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "# line 667\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "\n",
    "# line 706\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts, # concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "```\n",
    "Now, we know how the input of the `__call__` method of `FlaxDataCollatorForT5MLM` look like, let's dive into\n",
    "its implementation details:\n",
    "\n",
    "```python\n",
    "# list of dicts to dict of batched tensors of the same key, \n",
    "# BatchEncoding: https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.BatchEncoding\n",
    "# a wrapper of input data\n",
    "batch = BatchEncoding(\n",
    "    {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    ")\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "batch_size, expandend_input_length = input_ids.shape\n",
    "```\n",
    "Some interesting stuff happens below: given the length of a input sequence, we are deciding the indices\n",
    "of the tokens to be masked. Note that for span-mask language modeling, the masked tokens need to be \n",
    "locally-connected (locally-contiguous)\n",
    "\n",
    "```python\n",
    "mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n",
    "labels_mask = ~mask_indices\n",
    "```\n",
    "\n",
    "Let's look at how `def random_spans_noise_mask` is being implemented. Note that the implementation is a clone\n",
    "from [google's origal implementation](https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682).\n",
    "\n",
    "\n",
    "### Some auxilliary functions I will need to know\n",
    "\n",
    "#### np.stack\n",
    "If we have a list of $m$ arrays $A_0, \\cdots, A_{m-1}$ of shape $(x_0, x_1, \\cdots, x_n)$, then the result of the stack is an array \n",
    "with dimension $n+2$. If we stack then along axis 0, then the result would be $X$ such that\n",
    "$$\n",
    "X[i,...] = A_i\n",
    "$$\n",
    "If we stack along axis 1, then the result array $X$ would satisfy:\n",
    "$$\n",
    "X[:,i,...] = A_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask indices:  [False  True False  True False  True False False False]\n",
      "first_in_segment:  [False False  True False  True False  True False False False]\n",
      "segment id:  [0 0 1 1 2 2 3 3 3 3]\n",
      "segment_length:  [2 2 2 4]\n",
      "noise_span_lengths:  [1 5 5 1 3]\n",
      "nonnoise span length:  [ 3 10  4 11 57]\n",
      "interleved_span_lengths:  [ 3  1 10  5  4  5 11  1 57  3]\n",
      "span_starts [ 3  4 14 19 23 28 39 40 97]\n",
      "is_noise:  [False False False  True False False False False False False False False\n",
      " False False  True  True  True  True  True False False False False  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataConfig:\n",
    "    noise_density : float = 0.15 # mask language probaility\n",
    "    mean_noise_span_length : float = 3.0 # lenght of the noise span\n",
    "\n",
    "def random_span_noise_mask(data_config: DataConfig, length:int):\n",
    "    \"\"\"\n",
    "    Noise mask consisting of random spans of noise tokens.\n",
    "    The number of noise tokens and the number of noise spans and non-noise spans\n",
    "    are determined deterministically as follows:\n",
    "    num_noise_tokens = round(length * noise_density)\n",
    "    num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n",
    "    Spans alternate between non-noise and noise, beginning with non-noise.\n",
    "    Subject to the above restrictions, all masks are equally likely.\n",
    "\n",
    "    Args:\n",
    "        length: an int32 scalar (length of the incoming token sequence)\n",
    "        noise_density: a float - approximate density of output mask\n",
    "        mean_noise_span_length: a number\n",
    "\n",
    "    Returns:\n",
    "        a boolean tensor with shape [length] \n",
    "    \"\"\"\n",
    "    assert length > 1\n",
    "    orig_length = length \n",
    "\n",
    "    num_noise_tokens = int(np.round(length * data_config.noise_density))\n",
    "\n",
    "    # we want the number of noise and non-noise token to be positive\n",
    "    num_noise_tokens = min(max(num_noise_tokens, 1), length-1)\n",
    "\n",
    "    # np.round(0.5) = 0; np.round(0.51) = 1.0\n",
    "    num_noise_spans = int(np.round(num_noise_spans/ data_config.mean_noise_span_length))\n",
    "\n",
    "    # want positive number of spans\n",
    "    num_noise_spans = max(num_noise_spans, 1)\n",
    "    num_nonnoise_tokens = length - num_noise_tokens\n",
    "\n",
    "    # pick length of noise and non-noise spans\n",
    "    def _random_segmentation(num_items: int, num_segments: int):\n",
    "        mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "        np.random.shuffle(mask_indices)\n",
    "        first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "        segment_id = np.cumsum(first_in_segment)\n",
    "        # count length of sub segments assuming that list is sorted\n",
    "        _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "        return segment_length\n",
    "    \n",
    "    noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "    nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "\n",
    "num_items = 10; num_segments = 4\n",
    "mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "np.random.shuffle(mask_indices)\n",
    "\n",
    "print(\"mask indices: \", mask_indices)\n",
    "first_in_segment = np.pad(mask_indices, [[1, 0]]) # before_1 = 1, after_1 = 0\n",
    "\n",
    "print(\"first_in_segment: \", first_in_segment)\n",
    "\n",
    "segment_id = np.cumsum(first_in_segment) # cumulative sum\n",
    "print(\"segment id: \", segment_id)\n",
    "\n",
    "# count the length of sub segments assuming that list is sorted\n",
    "_, segment_length = np.unique(segment_id, return_counts=True)\n",
    "print(\"segment_length: \", segment_length)\n",
    "\n",
    "\n",
    "# pick length of noise and non-noise spans\n",
    "def _random_segmentation(num_items: int, num_segments: int):\n",
    "    mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "    np.random.shuffle(mask_indices)\n",
    "    first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "    segment_id = np.cumsum(first_in_segment)\n",
    "    # count length of sub segments assuming that list is sorted\n",
    "    _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "    return segment_length\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "length=100; num_noise_tokens = 15; num_nonnoise_tokens = 85; num_noise_spans = 5\n",
    "noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "print(\"noise_span_lengths: \", noise_span_lengths)\n",
    "print(\"nonnoise span length: \", nonnoise_span_lengths)\n",
    "\n",
    "# span, nonspan, span, nonspan,...\n",
    "interleaved_span_lengths = np.reshape(\n",
    "    np.stack([nonnoise_span_lengths, noise_span_lengths], axis=1), [num_noise_spans * 2]\n",
    ")\n",
    "print(\"interleved_span_lengths: \", interleaved_span_lengths)\n",
    "\n",
    "span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n",
    "print(\"span_starts\", span_starts)\n",
    "\n",
    "span_start_indicator = np.zeros((length, ), dtype=np.int8)\n",
    "span_start_indicator[span_starts] = True\n",
    "\n",
    "span_num = np.cumsum(span_start_indicator)\n",
    "is_noise = np.equal(span_num % 2, 1)\n",
    "is_noise = is_noise[:length]\n",
    "print(\"is_noise: \", is_noise)\n",
    "# np.stack means to stack arrays along a prescribed axis\n",
    "# np.stack([[0, 1],[0, 1]], axis=0) = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am confused here, isn't noise suppose to mean the masked token? Then why so many tokens in the sequence is said to be noise? I guess something happens later corrects it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[False  True  True  True False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(100,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "span_start_indicator = tf.math.unsorted_segment_sum(\n",
    "    tf.ones_like(span_starts), span_starts, length\n",
    ")\n",
    "span_num = tf.cumsum(span_start_indicator)\n",
    "is_noise = tf.equal(span_num % 2, 1)\n",
    "print(is_noise[:length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output from the `FlaxDataCollatorForT5MLM`. \n",
    "One thing I can do is to simply look at the output from this data collator given 1 paragraph of English text.\n",
    "I can use the `t5-base` tokenizer and write a paragraph myself and directly feed it into the collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded_inputs_length:  10\n",
      "target_length: 4\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded_inputs_length:  110\n",
      "target_length: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1147.45it/s]\n",
      "#0:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "\n",
      "#0: 100%|██████████| 2/2 [00:00<00:00,  8.28ba/s]\n",
      "#1: 100%|██████████| 2/2 [00:00<00:00,  8.67ba/s]\n",
      "#3: 100%|██████████| 2/2 [00:00<00:00,  9.34ba/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "#2: 100%|██████████| 2/2 [00:00<00:00,  7.19ba/s]\n",
      "#0:   0%|          | 0/10 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "#0:  10%|█         | 1/10 [00:00<00:01,  4.52ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  20%|██        | 2/10 [00:00<00:01,  4.37ba/s]\n",
      "\n",
      "#0:  30%|███       | 3/10 [00:00<00:01,  4.70ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  40%|████      | 4/10 [00:00<00:01,  4.78ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  50%|█████     | 5/10 [00:01<00:00,  5.09ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  60%|██████    | 6/10 [00:01<00:00,  5.22ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  70%|███████   | 7/10 [00:01<00:00,  4.85ba/s]\n",
      "\n",
      "#0:  80%|████████  | 8/10 [00:01<00:00,  4.80ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  90%|█████████ | 9/10 [00:01<00:00,  4.83ba/s]\n",
      "#0: 100%|██████████| 10/10 [00:01<00:00,  5.24ba/s]\n",
      "#2: 100%|██████████| 10/10 [00:01<00:00,  5.21ba/s]\n",
      "#1: 100%|██████████| 10/10 [00:01<00:00,  5.16ba/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 10/10 [00:01<00:00,  5.02ba/s]\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00,  5.44ba/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00,  4.43ba/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00,  4.58ba/s]\n",
      "\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00,  4.39ba/s]\n",
      "#0:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0: 100%|██████████| 2/2 [00:00<00:00, 35.04ba/s]\n",
      "#1: 100%|██████████| 2/2 [00:00<00:00, 36.02ba/s]\n",
      "#3: 100%|██████████| 2/2 [00:00<00:00, 39.52ba/s]\n",
      "#2: 100%|██████████| 2/2 [00:00<00:00, 32.40ba/s]\n",
      "#0:   0%|          | 0/10 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0:  30%|███       | 3/10 [00:00<00:00, 23.26ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  70%|███████   | 7/10 [00:00<00:00, 26.40ba/s]\n",
      "\n",
      "#0: 100%|██████████| 10/10 [00:00<00:00, 28.53ba/s]\n",
      "\n",
      "#1: 100%|██████████| 10/10 [00:00<00:00, 27.01ba/s]\n",
      "#2: 100%|██████████| 10/10 [00:00<00:00, 27.90ba/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 10/10 [00:00<00:00, 26.70ba/s]\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00, 21.91ba/s]\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00, 18.32ba/s]\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00, 18.92ba/s]\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00, 19.46ba/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from itertools import chain\n",
    "#datasets.list_datasets()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import run_t5_mlm_flax\n",
    "import imp\n",
    "imp.reload(run_t5_mlm_flax)\n",
    "\n",
    "from run_t5_mlm_flax import (\n",
    "    FlaxDataCollatorForT5MLM,\n",
    "    compute_input_and_target_lengths\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"t5-small\", max_seq_length=512, model_max_length=512)\n",
    "\n",
    "max_seq_length = 100\n",
    "\n",
    "expanded_inputs_length, target_length=compute_input_and_target_lengths(max_seq_length, 0.15, 3)\n",
    "\n",
    "print(\"expanded_inputs_length: \", expanded_inputs_length)\n",
    "print(\"target_length:\", target_length)\n",
    "data_collator = FlaxDataCollatorForT5MLM(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_density=0.15,\n",
    "    mean_noise_span_length=3,\n",
    "    input_length=max_seq_length,\n",
    "    target_length=target_length,\n",
    "    pad_token_id=0,\n",
    "    decoder_start_token_id=0\n",
    ")\n",
    "\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-v1\"\n",
    ")\n",
    "\n",
    "text_column_name=ds[\"train\"].column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], return_attention_mask=False)\n",
    "\n",
    "tokenized_ds = ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds[\"train\"].column_names[0],\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "\n",
    "expanded_inputs_length, targets_length = compute_input_and_target_lengths(\n",
    "    inputs_length=max_seq_length,\n",
    "    noise_density=0.15,\n",
    "    mean_noise_span_length=3\n",
    ")\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= expanded_inputs_length:\n",
    "        total_length = (total_length // expanded_inputs_length) * expanded_inputs_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + expanded_inputs_length] for i in range(0, total_length, expanded_inputs_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized_ds = tokenized_ds.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:10\t .</s> It met with positive sales in Japan, and was praised by both Japanese and western critics. After release, it received \n",
      "index:11\t downloadable content, along with an expanded edition in November of that year. It was also adapted into manga and an original video animation series. Due\n",
      "index:12\t to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan\n",
      "index:13\t translation compatible with the game's expanded edition was released in 2014. Media.Vision would return to the franchise with the development of Valkyri\n",
      "index:14\t a : Azure Revolution for the PlayStation 4.</s></s> = = Gameplay = =</s></s> As with previous<unk> Chronicles games, Valky\n",
      "index:15\t ria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy\n",
      "index:16\t forces. Stories are told through comic book @-@ like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and\n",
      "index:17\t partially through<unk> text. The player progresses through a series of linear missions, gradually unlocked as maps that can be freely<unk> through and\n",
      "index:18\t replayed as they are unlocked. The route to each story location on the map varies depending on an individual player's approach :\n",
      "index:19\t when one option is selected, the other is sealed off to the player. Outside missions, the player characters rest in a camp, where\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    print(\"index:%s\\t %s\" % (i,  tokenizer.decode(tokenized_ds[\"train\"][i][\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_span_lengths:  [2 4 1 5 4]\n",
      "nonnoise span length:  [ 3  6 22 55  8]\n",
      "interleved_span_lengths:  [ 3  2  6  4 22  1 55  5  8  4]\n",
      "span_starts [  3   5  11  15  37  38  93  98 106]\n",
      "span_start_indicator:  [0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      "span_num:  [0 0 0 1 1 2 2 2 2 2 2 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 8 8 8 8 8 8 8 8 9 9 9 9]\n",
      "is_noise,  [False False False  True  True False False False False False False  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True False False False False False False False False  True  True\n",
      "  True  True]\n",
      "noise_span_lengths:  [1 9 1 4 1]\n",
      "nonnoise span length:  [37  7 14  6 30]\n",
      "interleved_span_lengths:  [37  1  7  9 14  1  6  4 30  1]\n",
      "span_starts [ 37  38  45  54  68  69  75  79 109]\n",
      "span_start_indicator:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "span_num:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 6 6 6 6 6\n",
      " 6 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9]\n",
      "is_noise,  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True]\n",
      "noise_span_lengths:  [2 1 5 2 6]\n",
      "nonnoise span length:  [ 8  8 44 18 16]\n",
      "interleved_span_lengths:  [ 8  2  8  1 44  5 18  2 16  6]\n",
      "span_starts [  8  10  18  19  63  68  86  88 104]\n",
      "span_start_indicator:  [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "span_num:  [0 0 0 0 0 0 0 0 1 1 2 2 2 2 2 2 2 2 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9]\n",
      "is_noise,  [False False False False False False False False  True  True False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True]\n",
      "masked input:  Empire,<extra_id_0> by their benefactor <extra_id_1> of Maximilian or the chance to prove themselves in the war with Gallia, it is Da<extra_id_2>au's last<unk> card in creating a new Darcsen nation. As an armed Gallian force invading the Empire just following the two nations'cease @-@ fire would certainly wreck their newfound peace, Kurt<extra_id_3> make his squad the Nameless,<extra_id_4></s>\n",
      "masked label:  <extra_id_0> kept secret<extra_id_1>. Without the support<extra_id_2>h<extra_id_3> decides to once again<extra_id_4> asking Crowe to</s>\n",
      "\n",
      "masked input:  list himself and all under his command as killed @-@ in @-@ action. Now owing allegiance to none other than themselves, the 422nd confront<extra_id_0> Dahau and destroys the<extra_id_1> separate ways in order to begin their lives<unk>.</s></s> =<extra_id_2> Development = =</s></s> Concept<extra_id_3>ria Chronicles III began after development finished on Valkyria Chronicles II in early 2010, with full development beginning shortly after<extra_id_4></s>\n",
      "masked label:  <extra_id_0>s<extra_id_1><unk> weapon. Each member then goes their<extra_id_2> =<extra_id_3> work for Valky<extra_id_4> this</s>\n",
      "\n",
      "masked input:  . The director of Valkyr<extra_id_0> Chronicles II, Takeshi Oz<extra_id_1>, returned to that role for Valkyria Chronicles III. Development work took approximately one year. After the release of Valkyria Chronicles II, the staff took a<extra_id_2> response for the game and what they wanted to do next for the series. Like its<extra_id_3>, Valkyria Chronicles III was developed for PlayStation Portable :<extra_id_4></s>\n",
      "masked label:  <extra_id_0>ia<extra_id_1>awa<extra_id_2> look at both the popular<extra_id_3> predecessor <extra_id_4> this was due to the team</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [tokenized_ds[\"train\"][i] for i in [17, 18, 19]]\n",
    "masked_examples = data_collator(examples)\n",
    "\n",
    "for i in range(3):\n",
    "    m_input = tokenizer.decode(masked_examples[\"input_ids\"][i])\n",
    "    m_label = tokenizer.decode(masked_examples[\"labels\"][i])\n",
    "    print(\"masked input: \", m_input)\n",
    "    print(\"masked label: \", m_label)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masked input and output does seem to be what I expected. To make the logic of span-mask creation more transparent, I can excute the \n",
    "logics in `__call__` line by line and print out shape of each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_span_lengths:  [3 2 2 3 6]\n",
      "nonnoise span length:  [43  5 15 10 21]\n",
      "interleved_span_lengths:  [43  3  5  2 15  2 10  3 21  6]\n",
      "span_starts [ 43  46  51  53  68  70  80  83 104]\n",
      "span_start_indicator:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "span_num:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 2 2 2 2 2 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 6 6 6 6\n",
      " 6 6 6 6 6 6 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9]\n",
      "is_noise,  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True  True  True False False\n",
      " False False False  True  True False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False False False False False False False False  True  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True]\n",
      "noise_span_lengths:  [6 4 2 3 1]\n",
      "nonnoise span length:  [ 4  2 42 22 24]\n",
      "interleved_span_lengths:  [ 4  6  2  4 42  2 22  3 24  1]\n",
      "span_starts [  4  10  12  16  58  60  82  85 109]\n",
      "span_start_indicator:  [0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "span_num:  [0 0 0 0 1 1 1 1 1 1 2 2 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9]\n",
      "is_noise,  [False False False False  True  True  True  True  True  True False False\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True]\n",
      "noise_span_lengths:  [1 3 3 4 5]\n",
      "nonnoise span length:  [22 18 41  3 10]\n",
      "interleved_span_lengths:  [22  1 18  3 41  3  3  4 10  5]\n",
      "span_starts [ 22  23  41  44  85  88  91  95 105]\n",
      "span_start_indicator:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "span_num:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 5 5 5 6 6 6 7 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9]\n",
      "is_noise,  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True False False False  True  True  True  True False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True]\n",
      "mask indices:  [[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True  True  True False False\n",
      "  False False False  True  True False False False False False False False\n",
      "  False False False False False False False False  True  True False False\n",
      "  False False False False False False False False  True  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False  True  True  True  True\n",
      "   True  True]\n",
      " [False False False False  True  True  True  True  True  True False False\n",
      "   True  True  True  True False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True  True\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True  True\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False  True  True  True False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True  True  True False False False  True  True  True  True False\n",
      "  False False False False False False False False False  True  True  True\n",
      "   True  True]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BatchEncoding\n",
    "import numpy as np\n",
    "\n",
    "batch = BatchEncoding(\n",
    "    {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    ")\n",
    "\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "batch_size, expandend_input_length = input_ids.shape\n",
    "\n",
    "mask_indices = np.asarray([data_collator.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n",
    "labels_mask = ~mask_indices\n",
    "print(\"mask indices: \", mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_span_lengths:  [6 1 3 3 2]\n",
      "nonnoise span length:  [ 8 22 16 29 10]\n",
      "interleved_span_lengths:  [ 8  6 22  1 16  3 29  3 10  2]\n",
      "span_starts [ 8 14 36 37 53 56 85 88 98]\n",
      "span_start_indicator:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      "span_num:  [0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9]\n",
      "is_noise,  [False False False False False False False False  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True False False False False False False False False\n",
      " False False  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator.random_spans_noise_mask(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
