{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/workdir/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_no/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from t5_tokenizer_model import SentencePieceUnigramTokenizer\n",
    "\n",
    "vocab_size = 32000\n",
    "\n",
    "# size of the input data\n",
    "input_sentence_size = None\n",
    "\n",
    "cache_dir=\"/workdir/.cache/huggingface/datasets\"\n",
    "\n",
    "# using a tiny bit of the dataset to train a tokenizer\n",
    "ds = datasets.load_dataset(\"oscar\",  \n",
    "    name=\"unshuffled_deduplicated_no\", \n",
    "    cache_dir=cache_dir, \n",
    "    split=\"train[:100]\"\n",
    "    )\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "def batch_iterator(ds, input_sentence_size=None):\n",
    "    if input_sentence_size==None:\n",
    "        input_sentence_size = len(ds)\n",
    "    batch_size = 100\n",
    "    for i in range(0, input_sentence_size, batch_size):\n",
    "        yield ds[i: i+batch_size][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(ds),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "t5_config_dir = \"/workdir/norwegian-t5-base/\"\n",
    "if not os.path.exists(t5_config_dir):\n",
    "    os.makedirs(t5_config_dir)\n",
    "\n",
    "tokenizer.save(os.path.join(t5_config_dir, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6152"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import T5Config\n",
    "\n",
    "config = T5Config.from_pretrained(\"google/t5-v1_1-base\", vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained(t5_config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Config\n",
    "# use rust based tokenizer\n",
    "cache_dir=\"/workdir/norwegian-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cache_dir, \n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    use_auth_token=None\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config\n",
    "config = T5Config.from_pretrained(\n",
    "    \"/workdir/norwegian-t5-base/\",\n",
    "    cache_dir=cache_dir,\n",
    "    vocab_size = len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 experiments replicating\n",
    "I should answer the following\n",
    "questions when looking at the implementation:\n",
    "\n",
    "- how span-masking is implemented <\n",
    "- exactly how loss is computed.\n",
    "    - implementation details of decoder\n",
    "    - [do the most simple example](Dummy Examples)\n",
    "- input and labeled of span-masked pre-training data\n",
    "- modeling details / optimizer / learning rate scheduling\n",
    "- how they implemented metrics logging. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Span-masked language modeling\n",
    "\n",
    "Spans of the input sequence are masked by so-called sentinel tokens (unique mask tokens) and the output sequence\n",
    "is formed as a concatenation of the same sentinel tokens and the real token that has been masked out. For example, if the input sequence is \n",
    "\n",
    "`The bad dog ruined my sleep`\n",
    "\n",
    "We can mask out `bad dog` and ask the model to predict it. The sequence we will feed to the encoder is \n",
    "\n",
    "`The <extra_id_0> ruined my sleep`\n",
    "\n",
    "The label we use to compute the loss is \n",
    "\n",
    "`<extra_id_0> bad dog <extra_id_1>`\n",
    "\n",
    "T5-like span masked language models fuse the consecutively masked tokens to a single sentinel token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6152], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<extra_id_0>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad token id:  0\n",
      "decoder start token id:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"pad token id: \", config.pad_token_id)\n",
    "print(\"decoder start token id: \", config.decoder_start_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are needed to create span-masked \n",
    "\n",
    "- tokenizer: PreTrainedTokenizerBase\n",
    "A pretrained tokenizer with all the extra id stuff\n",
    "\n",
    "- noise_density: float = 0.15 (data_args.mlm_probability,)\n",
    "The probablity of mask out a token\n",
    "\n",
    "- mean_noise_span_length: float (data_args.mean_noise_span_length,)\n",
    "Average size of the masked span\n",
    "\n",
    "- input_length: int\n",
    "Maximum input sequence length. Defined as \n",
    "```\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "```\n",
    "\n",
    "- target_length: int\n",
    "Target sequence length. This quantity depends on the `max_seq_length` and is computed\n",
    "by `def compute_input_and_target_lengths`\n",
    "\n",
    "\n",
    "- pad_token_id: int (model.config.pad_token_id,)\n",
    "id for the pad token. 0 for T5\n",
    "\n",
    "- decoder_start_token_id: int (model.config.decoder_start_token_id)\n",
    "start token for sequence feed into decoder. 0 for T5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input type to `FlaxDataCollatorForT5MLM`\n",
    "Span-mask is implemeted in `FlaxDataCollatorForT5MLM`, the input to the `__call__` method is \n",
    "a `List[Dict[str, np.ndarray]]`, i.e. it is a batch of input data, each of them has the signiture:\n",
    "```\n",
    "{\n",
    "    \"input_ids\": [..., token ids,...],\n",
    "    \"masks\": np.array,\n",
    "}\n",
    "```\n",
    "The instance of `FlaxDataCollatorForT5MLM` is refered as\n",
    "`data_collator` in the code. I can check what kind of input is fed into `data_collator` at line 895. \n",
    "The `tokenized_dataset` object from which we generate the batch of data is a standard interface in HuggingFace\n",
    "\n",
    "### Hugging Face Dataset\n",
    "The `tokenized_dataset` is defined as follwing. This pattern is the same for many language training usages\n",
    "in Hugging Face\n",
    "```python\n",
    "# 557\n",
    "datasets = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    data_args.dataset_config_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "# line 667\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "\n",
    "# line 706\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts, # concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "```\n",
    "Now, we know how the input of the `__call__` method of `FlaxDataCollatorForT5MLM` look like, let's dive into\n",
    "its implementation details:\n",
    "\n",
    "```python\n",
    "# list of dicts to dict of batched tensors of the same key, \n",
    "# BatchEncoding: https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.BatchEncoding\n",
    "# a wrapper of input data\n",
    "batch = BatchEncoding(\n",
    "    {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    ")\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "batch_size, expandend_input_length = input_ids.shape\n",
    "```\n",
    "Some interesting stuff happens below: given the length of a input sequence, we are deciding the indices\n",
    "of the tokens to be masked. Note that for span-mask language modeling, the masked tokens need to be \n",
    "locally-connected (locally-contiguous)\n",
    "\n",
    "```python\n",
    "mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n",
    "labels_mask = ~mask_indices\n",
    "```\n",
    "\n",
    "Let's look at how `def random_spans_noise_mask` is being implemented. Note that the implementation is a clone\n",
    "from [google's origal implementation](https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlaxDataCollatorForT5MLM:\n",
    "    def __init__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
