{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset oscar (/workdir/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_no/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from t5_tokenizer_model import SentencePieceUnigramTokenizer\n",
    "\n",
    "vocab_size = 32000\n",
    "\n",
    "# size of the input data\n",
    "input_sentence_size = None\n",
    "\n",
    "cache_dir=\"/workdir/.cache/huggingface/datasets\"\n",
    "\n",
    "# using a tiny bit of the dataset to train a tokenizer\n",
    "ds = datasets.load_dataset(\"oscar\",  \n",
    "    name=\"unshuffled_deduplicated_no\", \n",
    "    cache_dir=cache_dir, \n",
    "    split=\"train[:100]\"\n",
    "    )\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "def batch_iterator(ds, input_sentence_size=None):\n",
    "    if input_sentence_size==None:\n",
    "        input_sentence_size = len(ds)\n",
    "    batch_size = 100\n",
    "    for i in range(0, input_sentence_size, batch_size):\n",
    "        yield ds[i: i+batch_size][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(ds),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "t5_config_dir = \"/workdir/norwegian-t5-base/\"\n",
    "if not os.path.exists(t5_config_dir):\n",
    "    os.makedirs(t5_config_dir)\n",
    "\n",
    "tokenizer.save(os.path.join(t5_config_dir, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6152"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import T5Config\n",
    "\n",
    "config = T5Config.from_pretrained(\"google/t5-v1_1-base\", vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained(t5_config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 experiments replicating\n",
    "I should answer the following\n",
    "questions when looking at the implementation:\n",
    "\n",
    "- how span-masking is implemented <\n",
    "- exactly how loss is computed.\n",
    "    - implementation details of decoder\n",
    "    - [do the most simple example](Dummy Examples)\n",
    "- input and labeled of span-masked pre-training data\n",
    "- modeling details / optimizer / learning rate scheduling\n",
    "- how they implemented metrics logging. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5Config\n",
    "# use rust based tokenizer\n",
    "cache_dir=\"/workdir/norwegian-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cache_dir, \n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    use_auth_token=None\n",
    ")\n",
    "\n",
    "config = T5Config.from_pretrained(\n",
    "    \"/workdir/norwegian-t5-base/\",\n",
    "    cache_dir=cache_dir,\n",
    "    vocab_size = len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Span-masked language modeling\n",
    "\n",
    "Spans of the input sequence are masked by so-called sentinel tokens (unique mask tokens) and the output sequence\n",
    "is formed as a concatenation of the same sentinel tokens and the real token that has been masked out. For example, if the input sequence is \n",
    "\n",
    "`The bad dog ruined my sleep`\n",
    "\n",
    "We can mask out `bad dog` and ask the model to predict it. The sequence we will feed to the encoder is \n",
    "\n",
    "`The <extra_id_0> ruined my sleep`\n",
    "\n",
    "The label we use to compute the loss is \n",
    "\n",
    "`<extra_id_0> bad dog <extra_id_1>`\n",
    "\n",
    "T5-like span masked language models fuse the consecutively masked tokens to a single sentinel token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6152], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<extra_id_0>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad token id:  0\n",
      "decoder start token id:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"pad token id: \", config.pad_token_id)\n",
    "print(\"decoder start token id: \", config.decoder_start_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are needed to create span-masked \n",
    "\n",
    "- tokenizer: PreTrainedTokenizerBase\n",
    "A pretrained tokenizer with all the extra id stuff\n",
    "\n",
    "- noise_density: float = 0.15 (data_args.mlm_probability,)\n",
    "The probablity of mask out a token\n",
    "\n",
    "- mean_noise_span_length: float (data_args.mean_noise_span_length,)\n",
    "Average size of the masked span\n",
    "\n",
    "- input_length: int\n",
    "Maximum input sequence length. Defined as \n",
    "```\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "```\n",
    "\n",
    "- target_length: int\n",
    "Target sequence length. This quantity depends on the `max_seq_length` and is computed\n",
    "by `def compute_input_and_target_lengths`\n",
    "\n",
    "\n",
    "- pad_token_id: int (model.config.pad_token_id,)\n",
    "id for the pad token. 0 for T5\n",
    "\n",
    "- decoder_start_token_id: int (model.config.decoder_start_token_id)\n",
    "start token for sequence feed into decoder. 0 for T5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input type to `FlaxDataCollatorForT5MLM`\n",
    "Span-mask is implemeted in `FlaxDataCollatorForT5MLM`, the input to the `__call__` method is \n",
    "a `List[Dict[str, np.ndarray]]`, i.e. it is a batch of input data, each of them has the signiture:\n",
    "```\n",
    "{\n",
    "    \"input_ids\": [..., token ids,...],\n",
    "    \"masks\": np.array,\n",
    "}\n",
    "```\n",
    "The instance of `FlaxDataCollatorForT5MLM` is refered as\n",
    "`data_collator` in the code. I can check what kind of input is fed into `data_collator` at line 895. \n",
    "The `tokenized_dataset` object from which we generate the batch of data is a standard interface in HuggingFace\n",
    "\n",
    "### Hugging Face Dataset\n",
    "The `tokenized_dataset` is defined as follwing. This pattern is the same for many language model training usages\n",
    "in Hugging Face\n",
    "```python\n",
    "# 557\n",
    "datasets = load_dataset(\n",
    "    data_args.dataset_name,\n",
    "    data_args.dataset_config_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "# line 667\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "\n",
    "# line 706\n",
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    group_texts, # concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n",
    "    batched=True,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")\n",
    "```\n",
    "Now, we know how the input of the `__call__` method of `FlaxDataCollatorForT5MLM` look like, let's dive into\n",
    "its implementation details:\n",
    "\n",
    "```python\n",
    "# list of dicts to dict of batched tensors of the same key, \n",
    "# BatchEncoding: https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.BatchEncoding\n",
    "# a wrapper of input data\n",
    "batch = BatchEncoding(\n",
    "    {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    ")\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "batch_size, expandend_input_length = input_ids.shape\n",
    "```\n",
    "Some interesting stuff happens below: given the length of a input sequence, we are deciding the indices\n",
    "of the tokens to be masked. Note that for span-mask language modeling, the masked tokens need to be \n",
    "locally-connected (locally-contiguous)\n",
    "\n",
    "```python\n",
    "mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n",
    "labels_mask = ~mask_indices\n",
    "```\n",
    "\n",
    "Let's look at how `def random_spans_noise_mask` is being implemented. Note that the implementation is a clone\n",
    "from [google's origal implementation](https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682).\n",
    "\n",
    "\n",
    "### Some auxilliary functions I will need to know\n",
    "\n",
    "#### np.stack\n",
    "If we have a list of $m$ arrays $A_0, \\cdots, A_{m-1}$ of shape $(x_0, x_1, \\cdots, x_n)$, then the result of the stack is an array \n",
    "with dimension $n+2$. If we stack then along axis 0, then the result would be $X$ such that\n",
    "$$\n",
    "X[i,...] = A_i\n",
    "$$\n",
    "If we stack along axis 1, then the result array $X$ would satisfy:\n",
    "$$\n",
    "X[:,i,...] = A_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask indices:  [ True False False False  True False False False  True]\n",
      "first_in_segment:  [False  True False False False  True False False False  True]\n",
      "segment id:  [0 1 1 1 1 2 2 2 2 3]\n",
      "segment_length:  [1 4 4 1]\n",
      "noise_span_lengths:  [1 5 5 1 3]\n",
      "nonnoise span length:  [ 3 10  4 11 57]\n",
      "interleved_span_lengths:  [ 1  3  5 10  5  4  1 11  3 57]\n",
      "span_starts [ 1  4  9 19 24 28 29 40 43]\n",
      "is_noise:  [False  True  True  True False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataConfig:\n",
    "    noise_density : float = 0.15 # mask language probaility\n",
    "    mean_noise_span_length : float = 3.0 # lenght of the noise span\n",
    "\n",
    "def random_span_noise_mask(data_config: DataConfig, length:int):\n",
    "    \"\"\"\n",
    "    Noise mask consisting of random spans of noise tokens.\n",
    "    The number of noise tokens and the number of noise spans and non-noise spans\n",
    "    are determined deterministically as follows:\n",
    "    num_noise_tokens = round(length * noise_density)\n",
    "    num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n",
    "    Spans alternate between non-noise and noise, beginning with non-noise.\n",
    "    Subject to the above restrictions, all masks are equally likely.\n",
    "\n",
    "    Args:\n",
    "        length: an int32 scalar (length of the incoming token sequence)\n",
    "        noise_density: a float - approximate density of output mask\n",
    "        mean_noise_span_length: a number\n",
    "\n",
    "    Returns:\n",
    "        a boolean tensor with shape [length] \n",
    "    \"\"\"\n",
    "    assert length > 1\n",
    "    orig_length = length \n",
    "\n",
    "    num_noise_tokens = int(np.round(length * data_config.noise_density))\n",
    "\n",
    "    # we want the number of noise and non-noise token to be positive\n",
    "    num_noise_tokens = min(max(num_noise_tokens, 1), length-1)\n",
    "\n",
    "    # np.round(0.5) = 0; np.round(0.51) = 1.0\n",
    "    num_noise_spans = int(np.round(num_noise_spans/ data_config.mean_noise_span_length))\n",
    "\n",
    "    # want positive number of spans\n",
    "    num_noise_spans = max(num_noise_spans, 1)\n",
    "    num_nonnoise_tokens = length - num_noise_tokens\n",
    "\n",
    "    # pick length of noise and non-noise spans\n",
    "    def _random_segmentation(num_items: int, num_segments: int):\n",
    "        mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "        np.random.shuffle(mask_indices)\n",
    "        first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "        segment_id = np.cumsum(first_in_segment)\n",
    "        # count length of sub segments assuming that list is sorted\n",
    "        _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "        return segment_length\n",
    "    \n",
    "    noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "    nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "\n",
    "num_items = 10; num_segments = 4\n",
    "mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "np.random.shuffle(mask_indices)\n",
    "\n",
    "print(\"mask indices: \", mask_indices)\n",
    "first_in_segment = np.pad(mask_indices, [[1, 0]]) # before_1 = 1, after_1 = 0\n",
    "\n",
    "print(\"first_in_segment: \", first_in_segment)\n",
    "\n",
    "segment_id = np.cumsum(first_in_segment) # cumulative sum\n",
    "print(\"segment id: \", segment_id)\n",
    "\n",
    "# count the length of sub segments assuming that list is sorted\n",
    "_, segment_length = np.unique(segment_id, return_counts=True)\n",
    "print(\"segment_length: \", segment_length)\n",
    "\n",
    "\n",
    "# pick length of noise and non-noise spans\n",
    "def _random_segmentation(num_items: int, num_segments: int):\n",
    "    mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "    np.random.shuffle(mask_indices)\n",
    "    first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "    segment_id = np.cumsum(first_in_segment)\n",
    "    # count length of sub segments assuming that list is sorted\n",
    "    _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "    return segment_length\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "length=100; num_noise_tokens = 15; num_nonnoise_tokens = 85; num_noise_spans = 5\n",
    "noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "print(\"noise_span_lengths: \", noise_span_lengths)\n",
    "print(\"nonnoise span length: \", nonnoise_span_lengths)\n",
    "\n",
    "# span, nonspan, span, nonspan,...\n",
    "interleaved_span_lengths = np.reshape(\n",
    "    np.stack([noise_span_lengths, nonnoise_span_lengths], axis=1), [num_noise_spans * 2]\n",
    ")\n",
    "print(\"interleved_span_lengths: \", interleaved_span_lengths)\n",
    "\n",
    "span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n",
    "print(\"span_starts\", span_starts)\n",
    "\n",
    "span_start_indicator = np.zeros((length, ), dtype=np.int8)\n",
    "span_start_indicator[span_starts] = True\n",
    "\n",
    "span_num = np.cumsum(span_start_indicator)\n",
    "is_noise = np.equal(span_num % 2, 1)\n",
    "is_noise = is_noise[:length]\n",
    "print(\"is_noise: \", is_noise)\n",
    "# np.stack means to stack arrays along a prescribed axis\n",
    "# np.stack([[0, 1],[0, 1]], axis=0) = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am confused here, isn't noise suppose to mean the masked token? Then why so many tokens in the sequence is said to be noise? I guess something happens later corrects it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[False  True  True  True False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      "  True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(100,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "span_start_indicator = tf.math.unsorted_segment_sum(\n",
    "    tf.ones_like(span_starts), span_starts, length\n",
    ")\n",
    "span_num = tf.cumsum(span_start_indicator)\n",
    "is_noise = tf.equal(span_num % 2, 1)\n",
    "print(is_noise[:length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output from the `FlaxDataCollatorForT5MLM`. \n",
    "One thing I can do is to simply look at the output from this data collator given 1 paragraph of English text.\n",
    "I can use the `t5-base` tokenizer and write a paragraph myself and directly feed it into the collator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
